---
title: "Report on Default of Credit Card Clients Dataset"
author: "Masayoshi Sato"
date: "2021/6/26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Finance is one of fields where machine learning is commonly used. It deals with a huge amount of data and is surrounded by a lot of uncertainties. To predict future, ranging from stock market prices to individual repayment of a debt, using pre-existing data is a crucial part of finance. In this paper, we will deal with a problem many credit card companies are facing. Can we predict whether a credit client will pay their debt or fail to pay? Traditionally, finding a credible borrower have been a kind of know-how, or skill and experience nurtured by experts. Instead, we try to form machine learning models. using a dataset which is open to public.

The dataset we use, "Default of Credit Card Clients Dataset" is stored in Kaggle website. It was collected in Taiwan in 2005. It has 24 variables, such as age, education, and payment condition. Outcome has two results, "0" non-default, "1" default. Each data was anonymously collected and labeled with individual IDs.

Our goal is to form a classification model which predicts the most accurate outcome, default or not. We need to bear it in mind that its distribution of these outcomes is imbalanced. Namely, the number of default clients are small compared to non- default clients. To address the issue, we will use not only accuracy but also other criteria, balanced accuracy.

We will use three machine learning models, logistic regression, decision tree, and random forest. If necessary, we will tune their parameters to find the best solution. Our procedures are as follows :

1.  Data exploration and data cleansing
2.  Splitting the dataset into train_set, validation_set, and test_set
3.  Applying models, logistic regression, decision tree, and random forest
4.  Considering models performance, and evaluating

This paper is written as a final assignment in "HarvardX PH125.9x Data Science: Capstone."

## Packages and Dataset

In this paper, we use following R packages, "tidyverse[^1]", "DataExplorer[^2]", "gridExtra[^3]", "rpart[^4]", "caret[^5]", and "ranger[^6]".

[^1]: <https://cran.r-project.org/web/packages/tidyverse/index.html>

[^2]: <https://cran.r-project.org/web/packages/DataExplorer/index.html>

[^3]: <https://cran.r-project.org/web/packages/gridExtra/index.html>

[^4]: <https://cran.r-project.org/web/packages/rpart/index.html>

[^5]: <https://cran.r-project.org/web/packages/caret/index.html>

[^6]: <https://cran.r-project.org/web/packages/ranger/index.html>

```{r Libraries used in this paper, warning=FALSE, include=FALSE}
if (!require(tidyverse)){install.packages("tidyverse")
library(tidyverse)
}

if (!require(DataExplorer)){install.packages("DataExplorer")
  library(DataExplorer)
}
#to do data exploration (correlation matrix etc.)

if (!require(gridExtra)){install.packages("gridExtra")
  library(gridExtra)
}
#expansion of ggplot

if (!require(caret)){install.packages("caret")
  library(caret)
}
#cross validation 

if(!require(rpart)) {install.packages("rpart")
  library(rpart)
}
#to make decision tree model

if (!require(rpart.plot)){install.packages("rpart.plot")
  library(rpart.plot)
}
#to plot decision tree


if (!require(ranger)){install.packages("ranger")
  library(ranger)
}
#random forest. new package that is much faster than "randomForest" package
```

We use a dataset stored in Kaggle[^7]website. In the description, it says, "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005." It is CSV file.

[^7]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

Kaggle requires registration to download the dataset. For the sake of convenience, the file is stored in my GitHub repository[^8].

[^8]: <https://github.com/masa951125/Final_project/raw/main/UCI_Credit_Card.csv>

```{r Downloading dataset, include=FALSE}

url <-"https://github.com/masa951125/Final_project/raw/main/UCI_Credit_Card.csv"
download.file(url, "original_default.csv")
original_default <- read_csv("original_default.csv")
```

## Data Exploration

First, we look at the downloaded dataset. Here are variables and their data type.

```{r columns, echo=FALSE}
str(original_default)
```

It has 30000 rows and 25 columns. "Default.payment.next.month" is our target variable. Some variables' values are limited number of integers. Some are rather wide range of numbers. "SEX", "EDUCATION", "MARRIAGE", "PAY_0" -"PAY_6", and "default.payment.next.month" look like categorical data.Other features seem to be numerical.

We know there are no NAs, Nulls in the dataset.

```{r echo=TRUE}
#Number of NAs 
sum(is.na(original_default))
```

```{r echo=TRUE}
#Number of Nulls
sum(is.null(original_default))
```

How these predictors are correlated? We use "plot_correlation" function to investigate this.

```{r Correlation Matrix, fig.height =6, fig.width=6,echo=FALSE}
plot_correlation(original_default) 
```

Takeaways from this are;

1.  Outcome (default.payment.next.month) has a strong positive correlation with PAY (positive), and PAY_AMT(negative), but a weak correlation with BILL_AMT.
2.  Overall, LIMIT_BAL has a relatively strong correlation with other factors (except SEX).
3.  EDUCATION, MARRIAGE, AGE have relatively strong correlation with one another.
4.  EDUCATION and AGE have a relatively weak correlation with PAY and BILL_AMT respectively.
5.  PAY and BILL_AMT, BILL_AMT and PAY_AMT have strong correlation.

We will look into individual features further.

### 1 Outcome

First, we look into the target variable, "default.payment.next.month". The data description says, "Default payment, 1=yes, 0=no.[^9]" We show the proportion of "0","1".

[^9]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r echo=FALSE}
prop.table(table(original_default$default.payment.next.month))
```

It is imbalanced. This means that if we predict all the outcome as "0", we will get 77.9% accuracy. We should be aware of this fact considering model's evaluation.

We change the name, "default.payment.next.month", to"DEFAULT" for the sake of convenience. Also, we change this numeric variable into factor.

```{r include=FALSE}
n <-which(names(original_default)=="default.payment.next.month")
names(original_default)[n] <- "DEFAULT"
```

```{r include=FALSE}
original_default$DEFAULT <- as.factor(original_default$DEFAULT)
```

### 2 "LIMIT_BAL"

This is an "amount of given credit in NT dollars (includes individual and family/supplementary credit)[^10]" NT stands for "New Taiwan". It is numerical data.

[^10]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset> NT stands for "New Taiwan".

```{r echo=FALSE}
summary(original_default$LIMIT_BAL)
```

We draw its distribution filling the proportion of default.

```{r LIMIT_BAL graph, echo=FALSE, fig.align="center", fig.height=3, fig.width=6}
ggplot(data=original_default, aes(LIMIT_BAL, fill=DEFAULT), ) +geom_histogram(bins=50)
```

Distribution is skewed right. Default clients seem to be gathered around lower range of LIMIT_BAL values.

### 3 "SEX"

The values "1", "2" correspond to male and female respectively[^11]. Male is approximately 40% and female is 60%.

[^11]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r SEX graph, echo=FALSE}
prop.table(table(original_default$SEX))
```

We draw its distribution and its proportion in terms of default rates.

```{r SEX distribution, echo=FALSE, fig.align="center", fig.height=3, fig.width=6}
sex_ditribution <- original_default %>% 
  ggplot(aes(x=as.factor(SEX), fill= DEFAULT)) +
  geom_bar() +
  ggtitle("SEX Distribution")
 
sex_stackedbar <-original_default %>% 
  ggplot(aes(x=as.factor(SEX), fill= DEFAULT)) +
  geom_bar(position="fill") +
  ggtitle("SEX Proportion") 

grid.arrange(sex_ditribution, sex_stackedbar, nrow=1, ncol=2)
```

There seemed to be little difference between genders. This categorical variable seems to be irrelevant to the outcome.

### 4 "EDUCATION"

In this variable, values are "1", "2", "3", "4", "5", "6". They are categorical values. The numbers have meanings as follows ; 1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown[^12]. We plot its distribution and stacked bar graph.

[^12]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r EDUCATION graph, echo=FALSE, fig.align="center", fig.height=3, fig.width=6}
EDU_distribution <-original_default %>% 
  ggplot(aes(x=as.factor(EDUCATION), fill= DEFAULT)) +
  geom_bar() +
  ggtitle("EDUCATION Distribution")

EDU_stackedbar <-original_default %>% 
  ggplot(aes(x=as.factor(EDUCATION), fill= DEFAULT)) +
  geom_bar(position="fill") +
  ggtitle("EDUCATION Proportion")

grid.arrange(EDU_distribution, EDU_stackedbar, nrow=1, ncol=2)
```

People whose final education is high school have relatively high default rate. On the other hand, people whose final education is graduate school have low default rate.

### 5 "MARRIAGE"

This is also categorical data. The value number means clients' marital status[^13]. 1=married, 2=single, 3=others. But there are 54 individuals whose values are 0.

[^13]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r include=FALSE}
sum(original_default$MARRIAGE==0)
```

We draw its distribution graph and stacked bar graph.

```{r MARRIAGE graph, echo=FALSE, fig.align="center", fig.height=3, fig.width=6}
MAR_distribution <- original_default %>% 
  ggplot(aes(x=as.factor(MARRIAGE), fill= DEFAULT)) +
  geom_bar() +
  ggtitle("MARRIAGE Distribution")

MAR_proportion <-original_default %>% 
  ggplot(aes(x=as.factor(MARRIAGE), fill= DEFAULT)) +
  geom_bar(position="fill") +
  ggtitle("MARRIAGE Proportion")

grid.arrange(MAR_distribution, MAR_proportion, nrow=1, ncol=2)
```

Value 0 is negligible. Comparing married and single people, married people are a little more likely to default.

### 6 "AGE"

As its name indicates, it is numerical data.

```{r echo=FALSE}
summary(original_default$AGE)
```

We plots its distribution filling the bar with each age bin's default rate.

```{r AGE graph, echo=FALSE, fig.align="center", fig.height=6, fig.width=6}
AGE_distribution <- original_default %>%
  ggplot(aes(AGE, fill=DEFAULT)) +
  geom_histogram(bins=30)+
  ggtitle("AGE Distribution")

AGE_proportion <- original_default %>% 
  ggplot(aes(x=AGE, fill= DEFAULT)) +
  geom_bar(position="fill") +
  ggtitle("AGE Proportion")

grid.arrange(AGE_distribution, AGE_proportion, nrow=2, ncol=1)
```

The number of clients are decreasing as they get older. Regarding default clients, younger people in their early 20s and older age groups around 60s show higher proportions of default than other age groups.

### 7 "PAY"

Variable from PAY_0, PAY_2 \~PAY_6 have the same values, as is explained in the description[^14]. They are categorical data. PAY_0 means repayment status in September, 2005. Then go back in time by a month until April, 2005. Values are ;

[^14]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r echo=FALSE}
unique(original_default$PAY_0)
```

They mean; "-1"= pay duly, "1"= payment delay for 1 month, "2" = payment delay for 2 months, ... 9 = payment delay for 9 months and above. "0" and "-2" are not defined.

We draw PAY_0 distribution and stacked bar graph.

```{r PAY_0 graph, echo=FALSE, fig.height=6, fig.width=6}
PAY_distribution <-original_default %>% 
  ggplot(aes(x=as.factor(PAY_0), fill= DEFAULT)) +
  geom_bar() +
  ggtitle("PAY_0 Distribution")

PAY_stackedbar <- original_default %>% 
  ggplot(aes(x=as.factor(PAY_0), fill= DEFAULT)) +
  geom_bar(position="fill") +
  ggtitle("PAY_0 Proportion")

grid.arrange(PAY_distribution, PAY_stackedbar, nrow=2, ncol=1)
  
```

We are not sure what "-2" and "0" mean. Non-default clients distribution is centered around 0. Default clients one is left skewed. From these two graphs we understand that as payment delay becomes longer, clients are more likely to come to a default.

PAY_2 \~ PAY_6 's distributions are almost as the same as PAY_0. We show their distribution.

```{r PAY graphs, echo=FALSE, fig.height=4, fig.width=6}
#PAY_0
graph_p0 <-
  original_default %>% ggplot(aes(x=as.factor(PAY_0))) +
  geom_bar() +
  ggtitle("PAY_0")

#PAY_2
graph_p2 <-  
  original_default %>% ggplot(aes(x=as.factor(PAY_2))) +
  geom_bar() +
  ggtitle("PAY_2")

#PAY_3
graph_p3 <-  
  original_default %>% ggplot(aes(x=as.factor(PAY_3))) +
  geom_bar() +
  ggtitle("PAY_3")

#PAY_4
graph_p4 <-
original_default %>% ggplot(aes(x=as.factor(PAY_4))) +
  geom_bar() +
  ggtitle("PAY_4")

#PAY_5
graph_p5 <-
  original_default %>% ggplot(aes(x=as.factor(PAY_5))) +
  geom_bar() +
  ggtitle("PAY_5")

#PAY_6
graph_p6 <-
  original_default %>% ggplot(aes(x=as.factor(PAY_6))) +
  geom_bar() +
  ggtitle("PAY_6")


#to show graph side by side, we use "grid.arrange" function in "gridExtra" package
grid.arrange(graph_p0, graph_p2, graph_p3, graph_p4, graph_p5, graph_p6, nrow=2, ncol=3)
```

### 8 "BILL_AMT"

This variable means an amount of bill statement. BILL_AMT1 is a record in September, 2005[^15]. Then go back in time by a month until April, 2005. Likewise previous variables, we plot BILL_AMT distribution, filling each bar with its default rate. This is numerical variable.

[^15]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r echo=FALSE}
summary(original_default[,13:18]) 
       
```

Maximum values are fluctuating, but medians and means are decreasing from BILL_AMT1 to BILL_AMT6.

```{r BILL_AMT1 graph, echo=FALSE, fig.height=3, fig.width=6}
ggplot(data=original_default, aes(BILL_AMT1,fill= DEFAULT)) +
  geom_histogram(bins=30)
```

It is right skewed. From BILL_AMT1 to BILL_AMT6, their distributions are almost the same as are shown in following plots.

```{r BILL_AMT graphs, echo=FALSE, fig.height=6, fig.width=6, message=FALSE}
b1 <- ggplot(data=original_default, aes(BILL_AMT1)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
b2 <- ggplot(data=original_default, aes(BILL_AMT2)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
b3 <- ggplot(data=original_default, aes(BILL_AMT3)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
b4 <- ggplot(data=original_default, aes(BILL_AMT4)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
b5 <- ggplot(data=original_default, aes(BILL_AMT5)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
b6 <- ggplot(data=original_default, aes(BILL_AMT6)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

grid.arrange(b1,b2,b3,b4,b5,b6, nrow=2, ncol=3)
```

### 9 "PAY_AMT"

This variable means an amount of previous payment. PAY_AMT1 is an amount of previous payment in September, 2005[^16]. Likewise BILL_AMT, PAY_AMT goes back in time by a month from August to April, 2005 which is PAY_AMT6.This is numerical data.

[^16]: <https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset>

```{r echo=FALSE}
summary(original_default[,19:24])
```

Their maximum values are fluctuating, but medians are gradually decreasing from PAY_AMT1 to PAY_AMT6.

Here is PAY_AMT1's plot.

```{r PAY_AMT, echo=FALSE, fig.height=3, fig.width=6, message=FALSE}
ggplot(data=original_default, aes(PAY_AMT1,fill= DEFAULT)) +geom_histogram(bins=30)
```

As we have seen a distribution summary, it is right skewed significantly. Maximum amount is more than 850,000 NT dollars, but its mean and median are 5664 and 2100 respectively. Also, about 17% of clients' values are 0.

```{r echo=TRUE}
#proportion of clients whose PAY_AMT is 0
mean(original_default$PAY_AMT1==0)
```

From PAY_AMT1 to PAY_AMT6, their distributions follow the same trend as PAY_AMT1.

```{r PAY_AMT graphs, echo=FALSE, fig.height=6, fig.width=6, message=FALSE}
p1 <- ggplot(data=original_default, aes(PAY_AMT1)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
p2 <- ggplot(data=original_default, aes(PAY_AMT2)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p3 <- ggplot(data=original_default, aes(PAY_AMT3)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p4 <- ggplot(data=original_default, aes(PAY_AMT4)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p5 <- ggplot(data=original_default, aes(PAY_AMT5)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p6 <- ggplot(data=original_default, aes(PAY_AMT6)) +geom_histogram(bins=30)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(p1,p2,p3,p4,p5,p6, nrow=2, ncol=3)
```

## Data Preparation

Before going further, we will arrange our dataset to make it easier to investigate. First, we remove ID column, as it is irrelevant to the outcome.

```{r include=FALSE}
original_default <- original_default %>% select(-ID)
```

Currently our data is composed of numerical values. Categorical values need to be changed to a factor. As we saw in data exploration, SEX,EDUCATION,MARRIAGE, PAY_0\~PAY_6 are categorical values.

```{r include=FALSE}
original_default <- original_default %>%
  mutate(SEX = as.factor(SEX),
         EDUCATION = as.factor(EDUCATION),
         MARRIAGE = as.factor(MARRIAGE),
         PAY_0 = as.factor(PAY_0),
         PAY_2 = as.factor(PAY_2),
         PAY_3 = as.factor(PAY_3),
         PAY_4 = as.factor(PAY_4),
         PAY_5 = as.factor(PAY_5),
         PAY_6 = as.factor(PAY_6) )
```

Regarding numerical values, "LIMIT_BAL" ranges from 10000 to 1000000, but "AGE" from 21 to 79 as we saw in the previous section. When doing regression, these wide varieties of ranges of variables cause inaccuracy. Thus we standardize these variables, using following formula.

$$Transformed.Values = \frac{Values - Mean}{Standard.Deviation}$$

We get variables whose means are 0, and standard deviations are 1 using function "scale".

```{r include=FALSE}
cat_col <- c("SEX", "EDUCATION", "MARRIAGE",
             "PAY_0", "PAY_2", "PAY_3", "PAY_4", "PAY_5", "PAY_6", "DEFAULT")

#all columns
all_col <- names(original_default)

#numerical data columns
num_col <- all_col[-which(all_col %in% cat_col)]

#scaling numerical data
original_default[num_col] <-original_default %>% select(-all_of(cat_col)) %>% scale()
```

Then, we split the data into two. As we have relatively a large amount of data, 30000, the proportion we use is 80% and 20%. The smaller dataset will be used when evaluating a model at the final stage. We call it test_set.

```{r include=FALSE}
set.seed(2021, sample.kind = "Rounding")
index_1 <- createDataPartition(original_default$DEFAULT, p=0.2, list=F, times=1)
test_set <- original_default[index_1,]
pre_train_set <- original_default[-index_1,]
```

We are going to use decision tree, and random forest later. They can be tuned to produce improved results by finding hyperparameters. If we tune the model using hyperparameters again and again, this might cause overfitting. To avoid this , we split the larger dataset again and produce two datasets, "train_set" and "validation_set". Split proportion is the same as before, 80% and 20% respectively.

```{r message=FALSE, include=FALSE}
set.seed(2021, sample.kind = "Rounding")
index_2 <- createDataPartition(pre_train_set$DEFAULT, p=0.2, list=F, times=1)
validation_set <-pre_train_set[index_2,]
train_set <- pre_train_set[-index_2,]
```

The three datasets have the almost same outcome ratio.

```{r echo=FALSE}
tibble(outcome =c(0,1),
       "train_set"= prop.table(table(train_set$DEFAULT)),
       "validation_set" = prop.table(table(validation_set$DEFAULT)),
       "test_set" = prop.table(table(test_set$DEFAULT)))
```

## Model Analysis

### 1 Evaluation Metrics

Generally speaking, overall accuracy is used to evaluate a model. But this dataset has imbalanced proportion of outcomes as we explored. What consequences bring about? Guessing all responses are 0, we calculate its confusion matrix.

```{r echo=FALSE}
base_pred <-factor(numeric(length(validation_set$DEFAULT)),levels=c("0","1"))

confusionMatrix(base_pred, validation_set$DEFAULT)$table
```

From this confusion matrix, we know even such a guess produces fairly good results.

-   Accuracy (True positive + True Negative / Total) 0.7788

-   Sensitivity (True Positive / True Positive+ False Negative) 1.0

In contrast,

-   Specificity (True Negative / True Negative + False Positive) 0

-   Balanced accuracy (arithmetic mean of sensitivity and specificity) 0.5

```{r include=FALSE}
confusionMatrix(base_pred, validation_set$DEFAULT)$overall[1] #accuracy
confusionMatrix(base_pred, validation_set$DEFAULT)$byClass[1] #sensitivity
confusionMatrix(base_pred, validation_set$DEFAULT)$byClass[2] #specificity
confusionMatrix(base_pred, validation_set$DEFAULT)$byClass[11] #balanced accuracy
```

As such, the credit company falsely give credit to a lot of clients who will fail to repay a debt. The loss for the company would be huge. Therefore, our goal is to pursue more accurate accuracy than guessing all responses are 0, taking account of improving specificity. We will use both *accuracy* and *balanced accuracy* to evaluate each model.

### 2 Logistic Regression

Firstly, we choose logistic regression. Logistic regression is commonly used in binary classification problems (outcome is 0 and 1, or True and False). As it uses logistic transformed odds, it produces outcome ranging from 0 to 1.

We have 24 features in the dataset. There are many ways to deal with such many predictors. First, we will form a logistic regression model with fewer variables. In the data exploration, we understand predictors from PAY_0 to PAY_6 seem to be important. But PAY variables, BILL_AMT variables and PAY_AMT variables have strong correlation. In addition to this, BILL_AMTs seem to have weak correlation with DEFAULT, target variable.

From these, we narrow down variables. We leave out BILL_AMT2 to BILL_AMT6, PAY_AMT2 to PAY_AMT6, PAY_2 to PAY_6 as well. Then we predict responses using glm function. The result;

```{r echo=FALSE, message=FALSE, warning=FALSE}
glm_fewer_mdl <- glm(DEFAULT ~ LIMIT_BAL + SEX + EDUCATION + MARRIAGE + AGE + 
                     PAY_0 +  PAY_AMT1 +                         BILL_AMT1,
                     data= train_set, family= binomial(link = "logit"))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(glm_fewer_mdl)
```

From this, we understand that;

-   Important variables :

    LIMIT_BAL, SEX(whether a client is a female or not), PAY_0( whether its values are 0, 1, 2, 3, 4), , BILL_AMT1,PAY_AMT1

Then fit the model to the validation set and show its confusion matrix.

```{r echo=TRUE, message=FALSE, warning=TRUE}
glm_fewer_prob <- predict(glm_fewer_mdl, validation_set,type="response")
glm_fewer_pred <- ifelse(glm_fewer_prob >0.5,1,0)
confusionMatrix(as.factor(glm_fewer_pred), validation_set$DEFAULT)$table
```

Other statistic metrics are;

```{r echo=FALSE}
glm_f_results <- tibble(method = "glm fewer features",
                  Accuracy =confusionMatrix(as.factor(glm_fewer_pred), validation_set$DEFAULT)$overall[1],
                  Sensitivity =confusionMatrix(as.factor(glm_fewer_pred), validation_set$DEFAULT)$byClass[1],
                  Specificity =confusionMatrix(as.factor(glm_fewer_pred), validation_set$DEFAULT)$byClass[2],
                  Balanced_Accuracy =confusionMatrix(as.factor(glm_fewer_pred), validation_set$DEFAULT)$byClass[11])
                  
glm_f_results %>% knitr::kable()
```

One of other ways is step-wise regression. This method is to repeatedly examine statistical significance of each variable in a regression model[^17]. There are three methods; forward selection, backward elimination, forward and backward elimination. Forward selection starts from no variable and add a variable step by step. Backward elimination starts from full model and subtract variables one by one. Forward and backward combines the two. One of the most convenient features of this method is we can get logistic regression results without knowing details of variables in the dataset.

[^17]: It uses Akaike information criterion (AIC) .<https://en.wikipedia.org/wiki/Akaike_information_criterion>

We use forward and backward step wise regression, as it is commonly used. In the step-wise regression, we make two models, null model with no predictors and full model with all predictors at first. Then we use "step" function to conduct step-wise regression. The result;

```{r step-wise regression, message=FALSE, warning=FALSE, include=FALSE}
#a null model with no predictors
null_model <- glm(DEFAULT~1, data = train_set, family = binomial(link = "logit"))

#a full model using all of the potential predictors
full_model <- glm(DEFAULT~., data = train_set, family = binomial(link = "logit"))

#conduct step-wise regression backward and forward
step_mdl   <- step(null_model, 
                   scope = formula(full_model), 
                   direction = "both") #in this case, forward and backward 
```

```{r echo=FALSE}
summary(step_mdl)
```

From this, we understand that;

-   Used variables:

    LIMIT_BAL, MARRIAGE, EDUCATION, SEX, PAY_0, PAY_3, PAY_4, PAY_5, PAY_6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT5, BILL_AMT3, BILL_AMT5

-   Important variables :

    PAY_0( whether its values are -1, 1, 2, 3, 4), PAY_3( whether its values are 2), PAY_6( whether its values are 0), LIMIT_BAL, BILL_AMT3, PAY_AMT1, PAY_AMT2, SEX(whether a client is a female or not)

Then fit the model to the validation set and make a confusion matrix.

```{r step-wise regression, echo=FALSE, warning=TRUE}
step_prob <- predict(step_mdl, validation_set,type="response")
step_pred <- ifelse(step_prob >0.5,1,0)
confusionMatrix(as.factor(step_pred), validation_set$DEFAULT)$table
```

Other statistic metrics are;

```{r echo=FALSE}
glm_s_results <- tibble(method = "glm step wise", 
                  Accuracy =confusionMatrix(as.factor(step_pred), validation_set$DEFAULT)$overall[1],
                  Sensitivity =confusionMatrix(as.factor(step_pred), validation_set$DEFAULT)$byClass[1],
                  Specificity =confusionMatrix(as.factor(step_pred), validation_set$DEFAULT)$byClass[2],
                  Balanced_Accuracy =confusionMatrix(as.factor(step_pred), validation_set$DEFAULT)$byClass[11])
                  
glm_s_results %>% knitr::kable()
```

Comparing the two models;

```{r echo=FALSE}
bind_rows(glm_f_results, glm_s_results) %>% knitr::kable()


```

The model with fewer features shows better accuracy than the step-wise model. But its specificity is worse then the step-wise model, in consequence produces worse balanced accuracy. We understand from these regression the selection of variables are crucial in determining the result. We look into other models.

### 3 Decision Tree

A decision tree is a familiar and intuitive method. For example, if you feel sick, a physician may ask you questions following a tree chart. You are asked whether it is yes or no at each node of the chart. After following the nodes, the doctor gets your health outcome.

Decision tree algorithm recursively divides the dataset into partitions with similar values for the outcome. A metric is used to choose the partitions. One of decision tree R functions "rpart" uses Gini index.

$$
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
$$

$\hat{p}_{j,k}$ as the proportion of observations in partition $\ {j}$ that are of class $\ {k}$.[^18] If there are lots of classes in the partition, Gini index, also called Gini impurity, increases up to 1. On the other hand, if there are few classes in the partition it decreases down to 0. If you predict perfectly, the index is 0.

[^18]: Irizarry, Rafael A, "Introduction to Data Science: Data Analysis and Prediction Algorithms with R, 31.10.4 Classification (decision) trees". *Internet archive*, <https://rafalab.github.io/dsbook/examples-of-algorithms.html#classification-decision-trees>

We use "rpart" function to make a model, then predict the outcome.

```{r include=FALSE}
set.seed(2021, sample.kind = "Rounding")
rpart_mdl <-rpart(DEFAULT ~ .,data = train_set)
```

```{r echo=FALSE}
rpart_mdl
```

We draw a decision tree based on this model.

```{r echo=FALSE, fig.height=3, fig.width=6}
rpart.plot(rpart_mdl)
```

This model uses only PAY_O to make partitions. This means the credit company need to check a client's PAY_0. If the value is -2, -1, 0, 1, 5, they are likely to repay, otherwise, they are likely to default on the debt.

Then predict the outcome, and show its confusion matrix.

```{r include=FALSE}
rpart_pred <- predict(rpart_mdl, validation_set, type="class")
```

```{r echo=FALSE}
confusionMatrix(rpart_pred, validation_set$DEFAULT)$table
```

Other statistic metrics;

```{r echo=FALSE}
rpart_results <- tibble(method = "rpart",
                  Accuracy =confusionMatrix(rpart_pred, validation_set$DEFAULT)$overall[1],
                  Sensitivity =confusionMatrix(rpart_pred, validation_set$DEFAULT)$byClass[1],
                  Specificity =confusionMatrix(rpart_pred, validation_set$DEFAULT)$byClass[2],
                  Balanced_Accuracy =confusionMatrix(rpart_pred, validation_set$DEFAULT)$byClass[11])
                  
rpart_results %>% knitr::kable()
```

Compared with the previous logistic regression models, its accuracy has improved but its balanced accuracy become worse.

We try to improve this result using other function "train" in caret package. When we use "train" , it conducts "cross validation" and finds optimal parameters in the decision tree algorithm.

One of the cross validation methods is k-fold cross validation. It splits the dataset into k "folds" randomly. For each group, it takes the group as a test dataset, and takes other groups as a training dataset. Then it fits a model on the training set and evaluates it on the test set. This procedure is repeated until every K-fold serve as the test set. Finally it summarizes the performance of the model as the average of these procedures.

"ModelLookup" function in caret package tells us what parameters in rpart we can tune.

```{r}
modelLookup("rpart")
```

Cp stands for complexity parameter. It determines the complexity of the model. As its value gets smaller, the algorithm produces more trees. In the previous decision tree model we fit, cp is 0.01. We want to know whether cp values less than 0.01 will improve the performance.

```{r eval=FALSE, include=FALSE}
rpart_mdl$control$cp
```

We train the dataset using "train" function. This function conducts 10 folds cross validation as a default. As we do not specify the number, it will conduct 10 folds cross validation.

```{r include=FALSE}
set.seed(2021, sample.kind = "Rounding")
rpart_tuned_mdl <- train(DEFAULT ~ ., 
                      method = "rpart", 
                      tuneGrid = data.frame(cp = seq(0, 0.01, len = 25)),
                      control = rpart.control(minsplit = 0),
                      data = train_set)
```

```{r echo=FALSE}
rpart_tuned_mdl
```

Plot cp.

```{r echo=FALSE, fig.height=3, fig.width=6}
plot(rpart_tuned_mdl)
```

Then draw a decision tree.

```{r echo=FALSE, fig.height=3, fig.width=6}
rpart.plot(rpart_tuned_mdl$finalModel)
```

As PAY variables are factored and turned into a dummy variable, a node "PAY_02", for example, is asking whether a client's PAY_0 is "-1" or not. "-1" is the second level in the PAY_0 original column.

Fit the model to the validation set and show its confusion matrix.

```{r echo=FALSE}
rpart_tuned_pred <- predict(rpart_tuned_mdl, validation_set)
confusionMatrix(rpart_tuned_pred, validation_set$DEFAULT)$table
```

Other statistic metrics;

```{r echo=FALSE}
rpart_tuned_results <- tibble(method = "rpart tuned",
                  Accuracy =confusionMatrix(rpart_tuned_pred, validation_set$DEFAULT)$overall[1],
                  Sensitivity =confusionMatrix(rpart_tuned_pred, validation_set$DEFAULT)$byClass[1],
                  Specificity =confusionMatrix(rpart_tuned_pred, validation_set$DEFAULT)$byClass[2],
                  Balanced_Accuracy =confusionMatrix(rpart_tuned_pred, validation_set$DEFAULT)$byClass[11])
                  
rpart_tuned_results %>% knitr::kable()
```

Comparing two decision models, the tuned model has improved in terms of specificity and balanced accuracy. But both performed worse than logistic regression using fewer predictors in terms of balanced accuracy.

```{r echo=FALSE}
bind_rows(rpart_results, rpart_tuned_results) %>% knitr::kable()

```

### 4 Random Forest

Decision tree algorithm is easy to understand as it follow human decision making process. However, it "can easily over-train" and "is not very flexible."[^19] To improve this, we introduce random forest algorithm.

[^19]: Irizarry, Rafael A, "Introduction to Data Science: Data Analysis and Prediction Algorithms with R, 31.10.4 Classification (decision) trees". *Internet archive*, [\<https://rafalab.github.io/dsbook/examples-of-algorithms.html\#classification-decision-trees\>](https://rafalab.github.io/dsbook/examples-of-algorithms.html#classification-decision-trees){.uri}

Random forest is one of the most popular machine learning algorithms. As its name implies, it makes multiple trees "**randomly** different" from a dataset, then produce a final prediction based on the average prediction of "combination of trees", namely "**forest**".[^20]

[^20]: Irizarry, Rafael A, "Introduction to Data Science: Data Analysis and Prediction Algorithms with R, 31.11 Random forests". *Internet archive*, <https://rafalab.github.io/dsbook/examples-of-algorithms.html#random-forests>

In this paper, we use "ranger" function. And show the model details;

```{r include=FALSE}
set.seed(2021, sample.kind = "Rounding")
rf_mdl <- ranger(
  formula = DEFAULT ~ ., 
  data = train_set,
  importance = "impurity",#use gini index
  probability = F)
```

```{r echo=FALSE}
rf_mdl
```

Plot the variables' importance;

```{r echo=FALSE, fig.height=3, fig.width=6}
imp_df <- data.frame(Variable = names(rf_mdl$variable.importance),
                   Importance = as.numeric(rf_mdl$variable.importance)) 

ggplot(imp_df, aes(x=Variable, y=Importance)) +
  geom_bar(stat="identity") +coord_flip()
```

As we saw in the decision tree model, PAY_0 is considerably important compared with other features. As for other features, BILL_AMT1, PAY_AMT1, and LIMIT_BAL, and AGE are important.

Then fit the model to the validation set and show its confusion matrix.

```{r echo=FALSE}
rf_pred <- predict(rf_mdl, validation_set)$predictions
confusionMatrix(rf_pred, validation_set$DEFAULT)$table
```

Other statistic metrics;.

```{r echo=FALSE}
rf_results <- tibble(method = "random forest",
                  Accuracy =confusionMatrix(rf_pred, validation_set$DEFAULT)$overall[1],
                  Sensitivity =confusionMatrix(rf_pred, validation_set$DEFAULT)$byClass[1],
                  Specificity =confusionMatrix(rf_pred, validation_set$DEFAULT)$byClass[2],
                  Balanced_Accuracy =confusionMatrix(rf_pred, validation_set$DEFAULT)$byClass[11])
                  
rf_results %>% knitr::kable()
```

The results has been much improved, especially specificity, compared to other models. Can we improve the result further? Using "modelLookup", we can sse parameters which can be tuned.

```{r echo=FALSE}
modelLookup("ranger")
```

Mtry is a number of variables to possibly split at in each node. Splitrule is a splitting rule. Min.node.size is a minimal node size. In the previous random forest model, its mtry is 4, we used Gini index as a splitting rule, and minimal node size is 1. In the following model, we compare mtry ranging from 3 to 10 in terms of accuracy.

```{r include=FALSE}
set.seed(2021, sample.kind = "Rounding")
rf_cv_mdl <- train( DEFAULT~ .,
                    data = train_set,
                    method = 'ranger',
                    metric = 'Accuracy', 
                    #we have compared accuracy with other models
                    importance = "impurity",
                    tuneGrid = expand.grid(
                    mtry = 3:10, splitrule = 'gini', min.node.size = 1), 
                    trControl = trainControl(method = 'cv'))

```

```{r echo=FALSE}
rf_cv_mdl
```

Plot mtry and accuracy.

```{r echo=FALSE, fig.height=3, fig.width=6}
plot(rf_cv_mdl)
```

Plot the variables' importance;

```{r echo=FALSE, fig.height=10, fig.width=6}
imp_cv_df <- data.frame(Variable = names(rf_cv_mdl$finalModel$variable.importance),
                   Importance = as.numeric(rf_cv_mdl$finalModel$variable.importance))

ggplot(imp_cv_df, aes(x=Variable, y=Importance)) +
  geom_bar(stat="identity") +coord_flip()
```

Contrary to the previous model, not PAY_0 but PAY_02's importance is outstanding.

Then fit the model to the validation set and show confusion matrix;

```{r echo=FALSE}
rf_cv_pred <- predict(rf_cv_mdl, validation_set)
confusionMatrix(rf_cv_pred, validation_set$DEFAULT)$table
```

Other statistic metrics;

```{r echo=FALSE}
rf_tuned_results <- tibble(method = "random forest tuned",
                  Accuracy =confusionMatrix(rf_cv_pred, validation_set$DEFAULT)$overall[1],
                  Sensitivity =confusionMatrix(rf_cv_pred, validation_set$DEFAULT)$byClass[1],
                  Specificity =confusionMatrix(rf_cv_pred, validation_set$DEFAULT)$byClass[2],
                  Balanced_Accuracy =confusionMatrix(rf_cv_pred, validation_set$DEFAULT)$byClass[11])
                  
rf_tuned_results %>% knitr::kable()
```

Comparing these two random forest models, the results of the tuned model gets worse than the default model in terms of both accuracy and balanced accuracy. There occurred over fitting in the tuned model.

```{r echo=FALSE}
bind_rows(rf_results, rf_tuned_results) %>% knitr::kable()

```

## Evaluation

We look at the table in which the results of our 6 models are shown.

```{r echo=FALSE}
bind_rows(glm_s_results, glm_f_results, rpart_results, rpart_tuned_results, rf_results, rf_tuned_results)%>% knitr::kable()
```

Among them, the best performance in terms of both accuracy and balanced accuracy is produced by "**random forest default model**". As we mentioned before, **PAY_0** is the most important variables compared with others. Then follows **BILL_AMT1**, **PAY_AMT1**, and **LIMIT_BAL**, and **AGE.**

We fit this model to the test set and extract final results.

```{r echo=FALSE}
final_rf_pred <- predict(rpart_mdl, test_set,type="class")
```

Final statistic metrics are as follows;

```{r echo=FALSE}
final_results <- tibble( method ="final random forest",
                         Accuracy =confusionMatrix(final_rf_pred, test_set$DEFAULT)$overall[1],
                         Sensitivity =confusionMatrix(final_rf_pred, test_set$DEFAULT)$byClass[1],
                         Specificity =confusionMatrix(final_rf_pred, test_set$DEFAULT)$byClass[2],
                         Balanced_Accuracy = confusionMatrix(final_rf_pred, test_set$DEFAULT)$byClass[11]) %>% knitr::kable()
final_results
```

Fitting the model to the test set, we get **accuracy 0.8226962** and **balanced accuracy 0.6471006**. This results outperformed considerably our first guess (guessing all outcomes are 0), accuracy 0.7788 and balanced accuracy 0.5.

## Conclusion

### Summary of Our Research

In this paper, we used an open data, "Default of Credit Card Clients Dataset" and explored its information seeking for insights which contributed to the model making. We used three methods, logistic regression, decision tree, and random forest. In each method, we tried to improve the results by changing variables and tuning hyperparameters.

Random forest algorithm showed the best performance. Fitting it to the test set, we got accuracy 0.8226962 and balanced accuracy 0.6471006. But when tuned, the model showed over-fitted results. The logistic regression models outperform the decision tree models. As well as the random forest models, the default decision tree model showed better performance than the tuned model.

### Future Study

As we use rather simple methods in the paper, we recognized our limitations in terms of dealing with the variables. Basically, we formed logistic regression model assuming that independent variables are not highly correlated with each other. We eliminated some of the variables according to some insights from our exploration. But we need to use more rigorous methods regarding the variables. Tweaking the variables or regularization should be taken into consideration. Moreover, the variables, PAY, PAY_AMT,and BILL_AMT, are a kind of historical data over a half year. If we can look into these historical changes further, we might be able to acquire more accurate results.

From a business point of view, overall economic condition may play a very important role in predicting people's financial behavior. A case in point, many people could not afford to pay their loan in the late 2000s due to the financial crisis at that time. On the other hand, when economy is booming, people repay the debt easily. Our dataset did not have such information, but if we want to make a more credible and practical model, we need to consider what kind of data is offered.
